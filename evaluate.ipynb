{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f657afe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os,sys\n",
    "sys.path.append('../')\n",
    "from os.path import join\n",
    "import glob\n",
    "import  random\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "# from face2speaker_main import Face2Speaker\n",
    "# from Recode.model import DoubleLayerModel, SingleLayerModel\n",
    "from senet import *\n",
    "from model import *\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2069f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "class Evaluation():\n",
    "    def __init__(self, \n",
    "                root, \n",
    "                embedder,\n",
    "                num_queries=100, \n",
    "                gallery_size=5, \n",
    "                distance_metric='euclidean'):\n",
    "\n",
    "        self.embedder = embedder\n",
    "        self.root = root\n",
    "        self.gallery_size = gallery_size\n",
    "        self.num_queries = num_queries\n",
    "        self.distance_metric= distance_metric\n",
    "        self.generate_eval_data()\n",
    "        pass\n",
    "\n",
    "    def distance(self,a,b):\n",
    "        if self.distance_metric=='euclidean':\n",
    "            return np.linalg.norm(a-b)\n",
    "        elif self.distance_metric=='cosine':\n",
    "            return spatial.distance.cosine(a,b)\n",
    "\n",
    "    def generate_eval_data(self):\n",
    "        _id_list = sorted(os.listdir(self.root))\n",
    "\n",
    "        queries = []\n",
    "        final_gallery = []\n",
    "\n",
    "        for _id in sorted(os.listdir(self.root)):\n",
    "            _id_path = join(self.root, _id)\n",
    "            for _url in sorted(os.listdir(_id_path)):\n",
    "                _url_path = join(_id_path,_url)\n",
    "                listOfAud=[f for f in os.listdir(os.path.join(_url_path, \"audio\"))]\n",
    "                for aud in sorted(listOfAud):\n",
    "                    emb = join(_url_path, \"audio\", aud)\n",
    "                    queries.append(emb)\n",
    "        random.shuffle(queries)\n",
    "        for _idx, query in enumerate(queries[0:self.num_queries]):\n",
    "            _id = query.split(os.sep)[-4]\n",
    "            same_flag = 1\n",
    "\n",
    "            while same_flag:\n",
    "                answer_set = glob.glob(join(self.root, _id, join('*', \"frames\",'*.jpg')))\n",
    "\n",
    "                answer = random.choice(answer_set)\n",
    "                if(not answer.split(os.sep)[-3]==query.split(os.sep)[-3]):\n",
    "                    same_flag = 0\n",
    "                \n",
    "            diff_speakers = [i for i in _id_list if i!=_id]\n",
    "            random.shuffle(diff_speakers)\n",
    "\n",
    "            assert _id not in diff_speakers\n",
    "\n",
    "            impostor_gallery = [] \n",
    "            \n",
    "            for imp in diff_speakers[0:self.gallery_size-1]:\n",
    "                imp_embeddings = glob.glob(join(self.root,imp,join('*','frames', '*.jpg')))\n",
    "                impostor_gallery.append(random.choice(imp_embeddings))\n",
    "                \n",
    "            impostor_gallery.append(answer)\n",
    "            final_gallery.append(impostor_gallery)\n",
    "        print(np.array(queries).shape)    \n",
    "        self.queries = np.array(queries[0:self.num_queries])\n",
    "        self.galleries = np.array(final_gallery[0:self.num_queries])\n",
    "        self.answer = np.array([self.gallery_size-1]*self.num_queries)\n",
    "        # print(\"self.queries\", self.queries)\n",
    "        # print(\"self.galleries\", self.galleries)\n",
    "        print(\"Num queries : %d\"%(len(self.queries)))\n",
    "        print(\"Gallery Size : %d\"%(self.galleries.shape[1]))\n",
    "        pass\n",
    "\n",
    "    def evaluate(self):\n",
    "\n",
    "        test_samples = self.num_queries\n",
    "\n",
    "        result = []\n",
    "        \n",
    "        for _idx, query in enumerate(self.queries[0:test_samples]):\n",
    "            distances=[]\n",
    "            for toMatch in self.galleries[_idx]:\n",
    "                face_emb, audio_emb=self.embedder.get_embedding(input_path_pair=(toMatch, query))\n",
    "                distances.append(self.distance(face_emb, audio_emb))\n",
    "            result.append(np.argmin(distances))    \n",
    "            \n",
    "        result = np.array(result)\n",
    "        r = len(np.where(result==self.answer[0:test_samples])[0])\n",
    "        accuracy = r/test_samples\n",
    "        print(\"Identification Accuracy : %.4f\"%(accuracy))\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ada25723",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetEmbeddings():\n",
    "    def __init__(self, \n",
    "                 learnable_pins_model):\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.learnable_pins_model = learnable_pins_model \n",
    "        self.learnable_pins_model.to(self.device)\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            self.learnable_pins_model = nn.DataParallel(self.learnable_pins_model) \n",
    "#         self.learnable_pins_model.module.test()\n",
    "        self.learnable_pins_model.eval()\n",
    "        \n",
    "    def get_embedding(self, input_path_pair=None, emb=None):\n",
    "        if input_path_pair[0] is not None and input_path_pair[1] is not None:\n",
    "            transformToTensor = transforms.ToTensor()         \n",
    "            face_frame = Image.open(input_path_pair[0]).convert('RGB')\n",
    "            audio_fft = np.load(input_path_pair[1])\n",
    "        img_transform = transforms.Compose([\n",
    "            # transforms.RandomHorizontalFlip(),\n",
    "            # transforms.ColorJitter(brightness=0.5, hue=0.3),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "#         display(face_frame)\n",
    "        face_frame = img_transform(face_frame).unsqueeze(0)\n",
    "        face_frame = face_frame.to(self.device)\n",
    "\n",
    "        audio_fft = transformToTensor(audio_fft).unsqueeze(0)\n",
    "        audio_fft = audio_fft.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            res_face_emb, res_audio_emb = self.learnable_pins_model(face_frame, audio_fft)\n",
    "        res_audio_emb = res_audio_emb.cpu().numpy().reshape(-1)\n",
    "        res_face_emb = res_face_emb.cpu().numpy().reshape(-1)\n",
    "\n",
    "        return res_face_emb, res_audio_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df891518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "senet\n",
      "Weights Loaded!\n",
      "(36237,)\n",
      "Num queries : 36237\n",
      "Gallery Size : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█▍        | 1/7 [44:26<4:26:40, 2666.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identification Accuracy : 0.7745\n",
      "senet\n",
      "Weights Loaded!\n",
      "(36237,)\n",
      "Num queries : 36237\n",
      "Gallery Size : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▊       | 2/7 [1:27:13<3:37:18, 2607.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identification Accuracy : 0.7770\n",
      "senet\n",
      "Weights Loaded!\n",
      "(36237,)\n",
      "Num queries : 36237\n",
      "Gallery Size : 2\n"
     ]
    }
   ],
   "source": [
    "def run_evaluate(root=\"/scratch/starc52/VoxCeleb2/test/mp4/\", model_path=join('/ssd_scratch/cvit/starc52/LPscheckpoints/', 'model_e34.pth')):\n",
    "    test_root = root\n",
    "\n",
    "    model = LearnablePINSenetVggVox256(training=False)\n",
    "    model.test()\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    model.load_state_dict(torch.load(model_path)['model_state_dict'])\n",
    "\n",
    "    embedder = GetEmbeddings(learnable_pins_model=model)\n",
    "\n",
    "\n",
    "    acc_arr = []\n",
    "#     for i in tqdm(range(2, 11)):\n",
    "    evaluation = Evaluation(root=test_root, embedder=embedder,gallery_size=2,num_queries=36237)\n",
    "\n",
    "    acc = evaluation.evaluate()\n",
    "    acc_arr.append(acc)\n",
    "\n",
    "    return acc_arr\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for i in tqdm(range(30, 37)):\n",
    "        test_acc=run_evaluate(root=\"/ssd_scratch/cvit/starc52/VoxCeleb2/test/mp4\", model_path=join('/ssd_scratch/cvit/starc52/LPscheckpoints/', 'model_e'+str(i)+'.pth'))\n",
    "#     dev_acc=run_evaluate(root=\"/ssd_scratch/cvit/starc52/VoxCeleb2/dev/mp4\", model_path=join('/ssd_scratch/cvit/starc52/LPscheckpoints_1/', 'model_e0.pth'))\n",
    "#     plt.plot(np.arange(0, len(test_acc))+2, test_acc, label=\"test\")\n",
    "#     plt.plot(np.arange(0, len(dev_acc))+2, dev_acc, label=\"dev\")\n",
    "#     plt.xlabel('Gallery Size')\n",
    "#     plt.ylabel('Identification Accuracy')\n",
    "#     plt.title('1:N F-V matching')\n",
    "#     plt.grid()\n",
    "#     plt.legend()\n",
    "#     plt.savefig('/home/starc52/audret/graphs/epoch_43.png')\n",
    "#     plt.clf()\n",
    "    # run_multiple_epochs(root=args.root)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
