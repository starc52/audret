{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "430f5817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/ssd_scratch/cvit/starc52/VoxCeleb2/dev/mp4'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-26497e5f2e9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0mAudioModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioInference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m \u001b[0mlistOfIdentities\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mspeaker_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#just 1000 user ids.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeaker_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/ssd_scratch/cvit/starc52/VoxCeleb2/dev/mp4'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from dataloader import * \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "from torchvision import transforms\n",
    "from utils.utils import *\n",
    "from tqdm import tqdm\n",
    "from scipy import spatial\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "from vggm import VGGM\n",
    "from resnet import *\n",
    "from senet import *\n",
    "import traceback\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "from facenet_pytorch import MTCNN\n",
    "from signal_utils import preprocess\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# class L2Norm(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(L2Norm, self).__init__()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x / torch.norm(x, dim=1, keepdim=True)\n",
    "#         return x\n",
    "\n",
    "# class VGGFace2Model(nn.Module):\n",
    "#     def __init__(self, \n",
    "#                 embedding_dim=4096,\n",
    "#                 arch_type='resnet'\n",
    "#             ):\n",
    "#         super(VGGFace2Model, self).__init__()\n",
    "#         print(arch_type)\n",
    "#         if arch_type=='resnet':\n",
    "#             self.backbone = resnet50('/home/starc52/audret/resnet.pth')\n",
    "#         else:\n",
    "#             self.backbone = senet50('/home/starc52/audret/senet.pth')\n",
    "        \n",
    "#         print(\"Weights Loaded!\")\n",
    "\n",
    "#         for param in self.backbone.parameters():\n",
    "#             # param.requires_grad = True\n",
    "#             param.requires_grad = False\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         batch_size = x.size(0)\n",
    "#         x = self.backbone(x)[1]\n",
    "#         x = x.view(batch_size, -1)\n",
    "#         # x = self.fc(x)\n",
    "        \n",
    "#         return x\n",
    "\n",
    "\n",
    "# class FaceFeatureExtractor(nn.Module):\n",
    "#     def __init__(self, \n",
    "#                 embedding_dim=256\n",
    "#             ):\n",
    "#         super(FaceFeatureExtractor, self).__init__()\n",
    "\n",
    "#         self.backbone = InceptionResnetV1(pretrained='vggface2')\n",
    "\n",
    "#         for param in self.backbone.parameters():\n",
    "#             param.requires_grad = False\n",
    "\n",
    "#         # self.fc = nn.Sequential(\n",
    "#         #     nn.Linear(512, embedding_dim),\n",
    "#         #     nn.ReLU(inplace=True)\n",
    "#         # )\n",
    "#         # self.activation = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.backbone(x)\n",
    "#         # x = self.fc(x)\n",
    "#         # x = x / torch.norm(x, dim=1, keepdim=True)\n",
    "#         # # x = self.activation(x)\n",
    "        \n",
    "#         return x\n",
    "\n",
    "# class ImageInference():\n",
    "#     def __init__(self, model_val_path, arch_type=\"senet\"):\n",
    "#         self.model_val_path = model_val_path\n",
    "#         self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#         print(self.device)\n",
    "\n",
    "#         if arch_type==\"facenet\":\n",
    "#             self.model = FaceFeatureExtractor()\n",
    "#             self.model.load_state_dict(torch.load(model_val_path))\n",
    "#         else:\n",
    "#             self.model = VGGFace2Model(arch_type=arch_type)\n",
    "\n",
    "#         print(\"Model weights loaded from\", model_val_path)\n",
    "#         self.model.to(self.device)\n",
    "#         self.model.eval()\n",
    "\n",
    "#         self.img_transform = ImageTransform(arch_type=arch_type)\n",
    "        \n",
    "\n",
    "#     def get_signature(self, img_path, save_path=None, img=None):\n",
    "#         # img = self.img_transform.transform(img_path, img=img)\n",
    "#         with torch.no_grad():\n",
    "#             dev_img = img.unsqueeze(0)\n",
    "#             batch_img = dev_img.to(self.device)\n",
    "#             embedding = self.model(batch_img)\n",
    "\n",
    "#         embedding = embedding.cpu().numpy()\n",
    "#         if save_path is not None:\n",
    "#             np.save(save_path, embedding)\n",
    "#             return\n",
    "#         return embedding\n",
    "\n",
    "class AudioInference():\n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(self.device)\n",
    "\n",
    "        # self.model=VGGM(1251)\n",
    "        # # self.model.load_state_dict(torch.load(\"../VGGVox-PyTorch-master/models/VGGM300_BEST_140_81.99.pth\", map_location=device))\n",
    "        # self.model.load_state_dict(torch.load(\"/home/starc52/audret/models/VGGM300_BEST_140_81.99.pth\", map_location=self.device))\n",
    "        # self.model.to(self.device)\n",
    "        # print(\"VggVox model loaded\")\n",
    "        # self.model.eval()\n",
    "        # self.preprocess = preprocess(arch_type=arch_type)\n",
    "        \n",
    "\n",
    "    def get_audio_features(self, audio_path, save_path=None, img=None):\n",
    "\n",
    "        audio,sr=librosa.load(audio_path,sr=16000)\n",
    "\n",
    "        audio = audio[0:48000]\n",
    "\n",
    "        audio = preprocess(audio).astype(np.float32)\n",
    "        audio=np.expand_dims(audio, 2)\n",
    "\n",
    "        transformers=transforms.ToTensor()\n",
    "        audio = transformers(audio)\n",
    "        audio = audio.unsqueeze(0)\n",
    "\n",
    "        audio = audio.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            audio_embedding = self.model(audio)\n",
    "\n",
    "        audio_embedding = audio_embedding.cpu().numpy()\n",
    "    \n",
    "        return audio_embedding\n",
    "\n",
    "    # todo\n",
    "    #takes audio as input, splits it with window and stride params, then comptues the embeddings\n",
    "    def split_audio(self, audio, window=3, stride=2, sr=16000) :\n",
    "        # window and stride is in seconds\n",
    "\n",
    "        if type(audio)==str:\n",
    "            pathToAudio = audio\n",
    "            audio,sr=librosa.load(audio,sr=16000)\n",
    "\n",
    "        nsamples_per_window = int(window*sr)\n",
    "        samples_per_stride = int(stride*sr)\n",
    "\n",
    "        slice_start = [i for i in range(0,len(audio)-nsamples_per_window,samples_per_stride)]\n",
    "        slice_end = [i+nsamples_per_window if i+nsamples_per_window<len(audio) else len(audio) for i in slice_start]\n",
    "        \n",
    "        audios = [audio[slice_start[i]:slice_end[i]] for i in range(len(slice_start))]\n",
    "        audios = random.sample(audios, 1)\n",
    "        audios_processed = [preprocess(audio).astype(np.float32) for audio in audios]\n",
    "        audios_expanded = [np.expand_dims(audio, 2) for audio in audios_processed]\n",
    "        try:\n",
    "            [np.save(join(pathToAudio[:-4]+\".npy\"), audio_fft) for _, audio_fft in enumerate(audios_expanded)]\n",
    "            return audios_expanded\n",
    "        except:\n",
    "            print(\"audio_fft directory already present\")\n",
    "        # transformers = transforms.ToTensor()\n",
    "        # audios_transformed = [transformers(audio) for audio in audios_expanded]\n",
    "        # audios_unsqzd = [audio.unsqueeze(0) for audio in audios_transformed]\n",
    "        # embeddings = []\n",
    "        # try:\n",
    "        #     os.makedirs(pathToAudio[:-4])\n",
    "        #     for idx, audio in enumerate(audios_unsqzd):\n",
    "        #         audio_temp = audio.to(self.device)\n",
    "        #         with torch.no_grad():\n",
    "        #             audio_embedding = self.model(audio_temp)\n",
    "        #         audio_embedding = audio_embedding.cpu().numpy()\n",
    "        #         np.save(join(pathToAudio[:-4], \"%03d.npy\"%idx), audio_embedding)\n",
    "        #         embeddings.append(audio_embedding)\n",
    "        # except:\n",
    "        #     print(\"directory present\")\n",
    "        \n",
    "        # return embeddings\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--root',default='/ssd_scratch/cvit/starc52/VoxCeleb2/dev/mp4', type=str)\n",
    "# parser.add_argument('--model_val_path',default=\"/home/starc52/Recode/senet.pth\", type=str)\n",
    "# parser.add_argument('--start_id', default=0, type=int)\n",
    "# parser.add_argument('--end_id', default=6000, type=int)\n",
    "# args = parser.parse_args()\n",
    "root = \"/ssd_scratch/cvit/starc52/VoxCeleb2/test/mp4\"\n",
    "model_val_path = \"/home/starc52/Recode/senet.pth\"\n",
    "start_id = 0\n",
    "end_id = 6000\n",
    "# print(args)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "mtcnn = MTCNN(post_process=False, keep_all=True, device=device)\n",
    "AudioModel = AudioInference()\n",
    "\n",
    "listOfIdentities=sorted(os.listdir(root))[start_id:end_id]\n",
    "for speaker_id in tqdm(sorted(os.listdir(root))[start_id:end_id]):#just 1000 user ids. \n",
    "    for url in sorted(os.listdir(join(root, speaker_id))):\n",
    "        # os.makedirs(join(save_path, speaker_id, url), exist_ok=True)\n",
    "        for file_name in sorted(os.listdir(join(root, speaker_id, url))):\n",
    "            try:\n",
    "                if file_name[-4:] == \".mp4\":\n",
    "                    cap = cv2.VideoCapture(join(root, speaker_id, url, file_name))\n",
    "                    transformToTensor = transforms.ToTensor()                    \n",
    "                    ret, img_file=cap.read()\n",
    "                    faces = mtcnn(img_file)\n",
    "                    if faces is None:\n",
    "                        count=0\n",
    "                        while cap.isOpened():\n",
    "                            ret, frame = cap.read()\n",
    "                    \n",
    "                            img_file=np.array(frame, copy=True)\n",
    "                            if not ret:\n",
    "                                break\n",
    "                            faces = mtcnn(img_file)\n",
    "                            if ret and faces is not None:\n",
    "                                cap.release()\n",
    "                                break\n",
    "                            count+=1\n",
    "                            cap.set(1, count)\n",
    "                    if faces is None:\n",
    "                        print(\"No face detected for : %s\" % join(root, speaker_id, url, file_name))\n",
    "                        os.remove(join(root, speaker_id, url, file_name))\n",
    "                        continue\n",
    "                    img_file = faces[0]\n",
    "                    img_file = img_file.permute(1, 2, 0).numpy()\n",
    "                    img_file  = cv2.resize(img_file, (224, 224))\n",
    "                    cv2.imwrite(join(root, speaker_id, url, file_name[:-4]+\".jpg\"), img_file)\n",
    "                    os.remove(join(root, speaker_id, url, file_name))\n",
    "                    print(\"Processed filename: %s\"% join(root, speaker_id, url, file_name))\n",
    "\n",
    "                elif file_name[-4:] == \".wav\":\n",
    "                    utteranceEmbedding = AudioModel.split_audio(join(root, speaker_id, url, file_name))\n",
    "                    os.remove(join(root, speaker_id, url, file_name))\n",
    "                    print(\"Processed filename: %s\"% join(root, speaker_id, url, file_name))\n",
    "            except:\n",
    "                print(traceback.format_exc())\n",
    "                continue            \n",
    "print(listOfIdentities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
