{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f657afe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os,sys\n",
    "sys.path.append('../')\n",
    "from os.path import join\n",
    "import glob\n",
    "import  random\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "# from face2speaker_main import Face2Speaker\n",
    "# from Recode.model import DoubleLayerModel, SingleLayerModel\n",
    "from senet import *\n",
    "from model import *\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62172d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_models():    \n",
    "    # getting the VGGFace_VGGM Learnable pins network\n",
    "    model_path=join('/ssd_scratch/cvit/starc52/LPscheckpoints','model_e35.pth')\n",
    "    model = LearnablePINSenetVggVox256()\n",
    "    model.test()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    model.load_state_dict(torch.load(model_path)['model_state_dict'])\n",
    "    print(\"Loaded Original Model\")\n",
    "    \n",
    "    # getting the distilled model for replacing VGG face image branch\n",
    "    distill_model_path=join('/ssd_scratch/cvit/starc52/distill_checkpoints','epoch_9.pth')\n",
    "    distill_model = CompleteDistillationModel(model)\n",
    "    distill_model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    distill_model.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        distill_model = nn.DataParallel(distill_model)\n",
    "    distill_model.load_state_dict(torch.load(distill_model_path)['model_state_dict'])\n",
    "    print(\"Loaded Distilled Model\")\n",
    "    \n",
    "    # initiating the learnable pins network for distilled image branch weights\n",
    "    print(\"Scaffolding for Compressed Distilled model\")\n",
    "    comb_model = LearnablePINSdistill()\n",
    "    print(\"Getting face FC\")\n",
    "    pre_face_fc = model.module.face_fc.state_dict()\n",
    "    print(\"Getting audio branch\")\n",
    "    pre_audio_model = model.module.audio_model.state_dict()\n",
    "    print(\"Getting audio FC\")\n",
    "    pre_audio_fc = model.module.audio_fc.state_dict()\n",
    "    print(\"Getting compression distilled face branch\")\n",
    "    pre_distill_model = distill_model.module.student_model.state_dict()\n",
    "\n",
    "    comb_model_face_fc = comb_model.face_fc.state_dict()\n",
    "    comb_model_audio_model = comb_model.audio_model.state_dict()\n",
    "    comb_model_audio_fc = comb_model.audio_fc.state_dict()\n",
    "    comb_model_distill_model = comb_model.face_model.state_dict()\n",
    "\n",
    "    print(\"Loading face FC layer\")\n",
    "    pre_face_fc = {k: v for k, v in pre_face_fc.items() if k in comb_model_face_fc}\n",
    "    comb_model_face_fc.update(pre_face_fc) \n",
    "    comb_model.face_fc.load_state_dict(comb_model_face_fc)\n",
    "    \n",
    "    print(\"Loading audio branch layer\")\n",
    "    pre_audio_model = {k: v for k, v in pre_audio_model.items() if k in comb_model_audio_model}\n",
    "    comb_model_audio_model.update(pre_audio_model) \n",
    "    comb_model.audio_model.load_state_dict(comb_model_audio_model)\n",
    "\n",
    "    print(\"Loading audio FC layer\")\n",
    "    pre_audio_fc = {k: v for k, v in pre_audio_fc.items() if k in comb_model_audio_fc}\n",
    "    comb_model_audio_fc.update(pre_audio_fc) \n",
    "    comb_model.audio_fc.load_state_dict(comb_model_audio_fc)\n",
    "\n",
    "    print(\"Loading compression distilled face branch layer\")\n",
    "    pre_distill_model = {k: v for k, v in pre_distill_model.items() if k in comb_model_distill_model}\n",
    "    comb_model_distill_model.update(pre_distill_model) \n",
    "    comb_model.face_model.load_state_dict(comb_model_distill_model)\n",
    "    # returning the new learnable pins model\n",
    "    return comb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2069f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "class Evaluation():\n",
    "    def __init__(self, \n",
    "                root, \n",
    "                embedder,\n",
    "                num_queries=100, \n",
    "                gallery_size=5, \n",
    "                distance_metric='euclidean'):\n",
    "\n",
    "        self.embedder = embedder\n",
    "        self.root = root\n",
    "        self.gallery_size = gallery_size\n",
    "        self.num_queries = num_queries\n",
    "        self.distance_metric= distance_metric\n",
    "        self.generate_eval_data()\n",
    "        pass\n",
    "\n",
    "    def distance(self,a,b):\n",
    "        if self.distance_metric=='euclidean':\n",
    "            return np.linalg.norm(a-b)\n",
    "        elif self.distance_metric=='cosine':\n",
    "            return spatial.distance.cosine(a,b)\n",
    "\n",
    "    def generate_eval_data(self):\n",
    "        _id_list = sorted(os.listdir(self.root))\n",
    "\n",
    "        queries = []\n",
    "        final_gallery = []\n",
    "\n",
    "        for _id in sorted(os.listdir(self.root)):\n",
    "            _id_path = join(self.root, _id)\n",
    "            for _url in sorted(os.listdir(_id_path)):\n",
    "                _url_path = join(_id_path,_url)\n",
    "                listOfAud=[f for f in os.listdir(os.path.join(_url_path, \"audio\"))]\n",
    "                for aud in sorted(listOfAud):\n",
    "                    emb = join(_url_path, \"audio\", aud)\n",
    "                    queries.append(emb)\n",
    "        random.shuffle(queries)\n",
    "        for _idx, query in enumerate(queries[0:self.num_queries]):\n",
    "            _id = query.split(os.sep)[-4]\n",
    "            same_flag = 1\n",
    "\n",
    "            while same_flag:\n",
    "                answer_set = glob.glob(join(self.root, _id, join('*', \"frames\",'*.jpg')))\n",
    "\n",
    "                answer = random.choice(answer_set)\n",
    "                if(not answer.split(os.sep)[-3]==query.split(os.sep)[-3]):\n",
    "                    same_flag = 0\n",
    "                \n",
    "            diff_speakers = [i for i in _id_list if i!=_id]\n",
    "            random.shuffle(diff_speakers)\n",
    "\n",
    "            assert _id not in diff_speakers\n",
    "\n",
    "            impostor_gallery = [] \n",
    "            \n",
    "            for imp in diff_speakers[0:self.gallery_size-1]:\n",
    "                imp_embeddings = glob.glob(join(self.root,imp,join('*','frames', '*.jpg')))\n",
    "                impostor_gallery.append(random.choice(imp_embeddings))\n",
    "                \n",
    "            impostor_gallery.append(answer)\n",
    "            final_gallery.append(impostor_gallery)\n",
    "        print(np.array(queries).shape)    \n",
    "        self.queries = np.array(queries[0:self.num_queries])\n",
    "        self.galleries = np.array(final_gallery[0:self.num_queries])\n",
    "        self.answer = np.array([self.gallery_size-1]*self.num_queries)\n",
    "        # print(\"self.queries\", self.queries)\n",
    "        # print(\"self.galleries\", self.galleries)\n",
    "        print(\"Num queries : %d\"%(len(self.queries)))\n",
    "        print(\"Gallery Size : %d\"%(self.galleries.shape[1]))\n",
    "        pass\n",
    "\n",
    "    def evaluate(self):\n",
    "\n",
    "        test_samples = self.num_queries\n",
    "\n",
    "        result = []\n",
    "        \n",
    "        for _idx, query in enumerate(self.queries[0:test_samples]):\n",
    "            distances=[]\n",
    "            for gal, toMatch in enumerate(self.galleries[_idx]):\n",
    "                face_emb, audio_emb=self.embedder.get_embedding(input_path_pair=(toMatch, query))\n",
    "                distances.append(self.distance(face_emb, audio_emb))\n",
    "            result.append(np.argmin(distances))\n",
    "            \n",
    "        result = np.array(result)\n",
    "        r = len(np.where(result==self.answer[0:test_samples])[0])\n",
    "        accuracy = r/test_samples\n",
    "        print(\"Identification Accuracy : %.4f\"%(accuracy))\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ada25723",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetEmbeddings():\n",
    "    def __init__(self, \n",
    "                 learnable_pins_model, loss_factor):\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.loss_factor = loss_factor\n",
    "        self.learnable_pins_model = learnable_pins_model \n",
    "        self.learnable_pins_model.to(self.device)\n",
    "        self.learnable_pins_model.eval()\n",
    "        \n",
    "    def get_embedding(self, input_path_pair=None, emb=None):\n",
    "        if input_path_pair[0] is not None and input_path_pair[1] is not None:\n",
    "            transformToTensor = transforms.ToTensor()         \n",
    "            face_frame = Image.open(input_path_pair[0]).convert('RGB')\n",
    "            audio_fft = np.load(input_path_pair[1])\n",
    "        img_transform = transforms.Compose([\n",
    "            transforms.Resize((int(224 * self.loss_factor), int(224 * self.loss_factor)), interpolation=2),\n",
    "            transforms.Resize((224, 224), interpolation=2),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        face_frame = img_transform(face_frame).unsqueeze(0)\n",
    "        face_frame = face_frame.to(self.device)\n",
    "\n",
    "        audio_fft = transformToTensor(audio_fft).unsqueeze(0)\n",
    "        audio_fft = audio_fft.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            res_face_emb, res_audio_emb = self.learnable_pins_model(face_frame, audio_fft)\n",
    "        res_audio_emb = res_audio_emb.cpu().numpy().reshape(-1)\n",
    "        res_face_emb = res_face_emb.cpu().numpy().reshape(-1)\n",
    "\n",
    "        return res_face_emb, res_audio_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df891518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "senet\n",
      "Weights Loaded!\n",
      "Loaded Original Model\n",
      "2048\n",
      "Weights Loaded!\n",
      "Loaded Distilled Model\n",
      "Scaffolding for Compressed Distilled model\n",
      "Getting face FC\n",
      "Getting audio branch\n",
      "Getting audio FC\n",
      "Getting compression distilled face branch\n",
      "Loading face FC layer\n",
      "Loading audio branch layer\n",
      "Loading audio FC layer\n",
      "Loading compression distilled face branch layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36237,)\n",
      "Num queries : 36237\n",
      "Gallery Size : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 1/9 [09:53<1:19:09, 593.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identification Accuracy : 0.7833\n",
      "(36237,)\n",
      "Num queries : 36237\n",
      "Gallery Size : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 2/9 [24:33<1:28:53, 761.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identification Accuracy : 0.6528\n",
      "(36237,)\n",
      "Num queries : 36237\n",
      "Gallery Size : 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 3/9 [43:58<1:34:37, 946.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identification Accuracy : 0.5500\n"
     ]
    }
   ],
   "source": [
    "def run_evaluate(root=\"/scratch/starc52/VoxCeleb2/test/mp4/\", loss_factor=0.25, model_path=join('/ssd_scratch/cvit/starc52/LPscheckpoints','model_e49.pth')):\n",
    "    test_root = root\n",
    "    \n",
    "    comb_model=combine_models()\n",
    "\n",
    "    embedder = GetEmbeddings(learnable_pins_model=comb_model, loss_factor=loss_factor)\n",
    "\n",
    "\n",
    "    acc_arr = []\n",
    "    for i in tqdm(range(2, 11)):\n",
    "        evaluation = Evaluation(root=test_root, embedder=embedder,gallery_size=i,num_queries=36237)\n",
    "\n",
    "        acc = evaluation.evaluate()\n",
    "        acc_arr.append(acc)\n",
    "\n",
    "    return acc_arr\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_acc=run_evaluate(root=\"/ssd_scratch/cvit/starc52/VoxCeleb2/test/mp4\", loss_factor=0.55, model_path=join('/ssd_scratch/cvit/starc52/LPscheckpoints/', 'model_e49.pth'))\n",
    "#     dev_acc=run_evaluate(root=\"/ssd_scratch/cvit/starc52/VoxCeleb2/dev/mp4\", loss_factor=0.25, model_path=join('/ssd_scratch/cvit/starc52/LPscheckpoints/', 'model_e47.pth'))\n",
    "    plt.plot(np.arange(0, len(test_acc))+2, test_acc, label=\"test\")\n",
    "    plt.plot(np.arange(0, len(dev_acc))+2, dev_acc, label=\"dev\")\n",
    "    plt.xlabel('Gallery Size')\n",
    "    plt.ylabel('Identification Accuracy')\n",
    "    plt.title('1:N F-V matching')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.savefig('/home/starc52/audret/graphs/epoch_43.png')\n",
    "    plt.clf()\n",
    "    # run_multiple_epochs(root=args.root)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
