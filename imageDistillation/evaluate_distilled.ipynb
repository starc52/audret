{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f657afe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os,sys\n",
    "sys.path.append('../')\n",
    "from os.path import join\n",
    "import glob\n",
    "import  random\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "# from face2speaker_main import Face2Speaker\n",
    "# from Recode.model import DoubleLayerModel, SingleLayerModel\n",
    "from senet import *\n",
    "from model import *\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62172d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_models():    \n",
    "    # getting the VGGFace_VGGM Learnable pins network\n",
    "    model_path=join('/ssd_scratch/cvit/starc52/LPscheckpoints','model_e35.pth')\n",
    "    model = LearnablePINSenetVggVox256()\n",
    "    model.test()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    model.load_state_dict(torch.load(model_path)['model_state_dict'])\n",
    "    print(\"Loaded Original Model\")\n",
    "    \n",
    "    # getting the distilled model for replacing VGG face image branch\n",
    "    distill_model_path=join('/ssd_scratch/cvit/starc52/distill_checkpoints','epoch_9.pth')\n",
    "    distill_model = CompleteDistillationModel(model)\n",
    "    distill_model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    distill_model.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        distill_model = nn.DataParallel(distill_model)\n",
    "    distill_model.load_state_dict(torch.load(distill_model_path)['model_state_dict'])\n",
    "    print(\"Loaded Distilled Model\")\n",
    "    \n",
    "    # initiating the learnable pins network for distilled image branch weights\n",
    "    print(\"Scaffolding for Compressed Distilled model\")\n",
    "    comb_model = LearnablePINSdistill()\n",
    "    print(\"Getting face FC\")\n",
    "    pre_face_fc = model.module.face_fc.state_dict()\n",
    "    print(\"Getting audio branch\")\n",
    "    pre_audio_model = model.module.audio_model.state_dict()\n",
    "    print(\"Getting audio FC\")\n",
    "    pre_audio_fc = model.module.audio_fc.state_dict()\n",
    "    print(\"Getting compression distilled face branch\")\n",
    "    pre_distill_model = distill_model.module.student_model.state_dict()\n",
    "\n",
    "    comb_model_face_fc = comb_model.face_fc.state_dict()\n",
    "    comb_model_audio_model = comb_model.audio_model.state_dict()\n",
    "    comb_model_audio_fc = comb_model.audio_fc.state_dict()\n",
    "    comb_model_distill_model = comb_model.face_model.state_dict()\n",
    "\n",
    "    print(\"Loading face FC layer\")\n",
    "    pre_face_fc = {k: v for k, v in pre_face_fc.items() if k in comb_model_face_fc}\n",
    "    comb_model_face_fc.update(pre_face_fc) \n",
    "    comb_model.face_fc.load_state_dict(comb_model_face_fc)\n",
    "    \n",
    "    print(\"Loading audio branch layer\")\n",
    "    pre_audio_model = {k: v for k, v in pre_audio_model.items() if k in comb_model_audio_model}\n",
    "    comb_model_audio_model.update(pre_audio_model) \n",
    "    comb_model.audio_model.load_state_dict(comb_model_audio_model)\n",
    "\n",
    "    print(\"Loading audio FC layer\")\n",
    "    pre_audio_fc = {k: v for k, v in pre_audio_fc.items() if k in comb_model_audio_fc}\n",
    "    comb_model_audio_fc.update(pre_audio_fc) \n",
    "    comb_model.audio_fc.load_state_dict(comb_model_audio_fc)\n",
    "\n",
    "    print(\"Loading compression distilled face branch layer\")\n",
    "    pre_distill_model = {k: v for k, v in pre_distill_model.items() if k in comb_model_distill_model}\n",
    "    comb_model_distill_model.update(pre_distill_model) \n",
    "    comb_model.face_model.load_state_dict(comb_model_distill_model)\n",
    "    # returning the new learnable pins model\n",
    "    return comb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2069f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "class Evaluation():\n",
    "    def __init__(self, \n",
    "                root, \n",
    "                embedder,\n",
    "                num_queries=100, \n",
    "                gallery_size=5, \n",
    "                distance_metric='euclidean'):\n",
    "\n",
    "        self.embedder = embedder\n",
    "        self.root = root\n",
    "        self.gallery_size = gallery_size\n",
    "        self.num_queries = num_queries\n",
    "        self.distance_metric= distance_metric\n",
    "        self.generate_eval_data()\n",
    "        pass\n",
    "\n",
    "    def distance(self,a,b):\n",
    "        if self.distance_metric=='euclidean':\n",
    "            return np.linalg.norm(a-b)\n",
    "        elif self.distance_metric=='cosine':\n",
    "            return spatial.distance.cosine(a,b)\n",
    "\n",
    "    def generate_eval_data(self):\n",
    "        _id_list = sorted(os.listdir(self.root))\n",
    "\n",
    "        queries = []\n",
    "        final_gallery = []\n",
    "\n",
    "        for _id in sorted(os.listdir(self.root)):\n",
    "            _id_path = join(self.root, _id)\n",
    "            for _url in sorted(os.listdir(_id_path)):\n",
    "                _url_path = join(_id_path,_url)\n",
    "                listOfAud=[f for f in os.listdir(os.path.join(_url_path, \"audio\"))]\n",
    "                for aud in sorted(listOfAud):\n",
    "                    emb = join(_url_path, \"audio\", aud)\n",
    "                    queries.append(emb)\n",
    "        random.shuffle(queries)\n",
    "        for _idx, query in enumerate(queries[0:self.num_queries]):\n",
    "            _id = query.split(os.sep)[-4]\n",
    "            same_flag = 1\n",
    "\n",
    "            while same_flag:\n",
    "                answer_set = glob.glob(join(self.root, _id, join('*', \"frames\",'*.jpg')))\n",
    "\n",
    "                answer = random.choice(answer_set)\n",
    "                if(not answer.split(os.sep)[-3]==query.split(os.sep)[-3]):\n",
    "                    same_flag = 0\n",
    "                \n",
    "            diff_speakers = [i for i in _id_list if i!=_id]\n",
    "            random.shuffle(diff_speakers)\n",
    "\n",
    "            assert _id not in diff_speakers\n",
    "\n",
    "            impostor_gallery = [] \n",
    "            \n",
    "            for imp in diff_speakers[0:self.gallery_size-1]:\n",
    "                imp_embeddings = glob.glob(join(self.root,imp,join('*','frames', '*.jpg')))\n",
    "                impostor_gallery.append(random.choice(imp_embeddings))\n",
    "                \n",
    "            impostor_gallery.append(answer)\n",
    "            final_gallery.append(impostor_gallery)\n",
    "        print(np.array(queries).shape)    \n",
    "        self.queries = np.array(queries[0:self.num_queries])\n",
    "        self.galleries = np.array(final_gallery[0:self.num_queries])\n",
    "        self.answer = np.array([self.gallery_size-1]*self.num_queries)\n",
    "        # print(\"self.queries\", self.queries)\n",
    "        # print(\"self.galleries\", self.galleries)\n",
    "        print(\"Num queries : %d\"%(len(self.queries)))\n",
    "        print(\"Gallery Size : %d\"%(self.galleries.shape[1]))\n",
    "        pass\n",
    "\n",
    "    def evaluate(self):\n",
    "\n",
    "        test_samples = self.num_queries\n",
    "\n",
    "        result = []\n",
    "        \n",
    "        for _idx, query in enumerate(self.queries[0:test_samples]):\n",
    "            distances=[]\n",
    "            for gal, toMatch in enumerate(self.galleries[_idx]):\n",
    "                face_emb, audio_emb=self.embedder.get_embedding(input_path_pair=(toMatch, query))\n",
    "                distances.append(self.distance(face_emb, audio_emb))\n",
    "            result.append(np.argmin(distances))\n",
    "            \n",
    "        result = np.array(result)\n",
    "        r = len(np.where(result==self.answer[0:test_samples])[0])\n",
    "        accuracy = r/test_samples\n",
    "        print(\"Identification Accuracy : %.4f\"%(accuracy))\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ada25723",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetEmbeddings():\n",
    "    def __init__(self, \n",
    "                 learnable_pins_model, loss_factor):\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.loss_factor = loss_factor\n",
    "        self.learnable_pins_model = learnable_pins_model \n",
    "        self.learnable_pins_model.to(self.device)\n",
    "        self.learnable_pins_model.eval()\n",
    "        \n",
    "    def get_embedding(self, input_path_pair=None, emb=None):\n",
    "        if input_path_pair[0] is not None and input_path_pair[1] is not None:\n",
    "            transformToTensor = transforms.ToTensor()         \n",
    "            face_frame = Image.open(input_path_pair[0]).convert('RGB')\n",
    "            audio_fft = np.load(input_path_pair[1])\n",
    "        img_transform = transforms.Compose([\n",
    "            transforms.Resize((int(224 * self.loss_factor), int(224 * self.loss_factor)), interpolation=2),\n",
    "            transforms.Resize((224, 224), interpolation=2),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        face_frame = img_transform(face_frame).unsqueeze(0)\n",
    "        face_frame = face_frame.to(self.device)\n",
    "\n",
    "        audio_fft = transformToTensor(audio_fft).unsqueeze(0)\n",
    "        audio_fft = audio_fft.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            res_face_emb, res_audio_emb = self.learnable_pins_model(face_frame, audio_fft)\n",
    "        res_audio_emb = res_audio_emb.cpu().numpy().reshape(-1)\n",
    "        res_face_emb = res_face_emb.cpu().numpy().reshape(-1)\n",
    "\n",
    "        return res_face_emb, res_audio_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df891518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "senet\n",
      "Weights Loaded!\n",
      "Loaded Original Model\n",
      "2048\n",
      "Weights Loaded!\n",
      "Loaded Distilled Model\n",
      "Scaffolding for Compressed Distilled model\n",
      "Getting face FC\n",
      "Getting audio branch\n",
      "Getting audio FC\n",
      "Getting compression distilled face branch\n",
      "Loading face FC layer\n",
      "Loading audio branch layer\n",
      "Loading audio FC layer\n",
      "Loading compression distilled face branch layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36237,)\n",
      "Num queries : 36237\n",
      "Gallery Size : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 1/9 [09:39<1:17:19, 579.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identification Accuracy : 0.7845\n",
      "(36237,)\n",
      "Num queries : 36237\n",
      "Gallery Size : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 2/9 [24:00<1:26:55, 745.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identification Accuracy : 0.6537\n",
      "(36237,)\n",
      "Num queries : 36237\n",
      "Gallery Size : 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 3/9 [43:03<1:32:40, 926.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identification Accuracy : 0.5515\n",
      "(36237,)\n",
      "Num queries : 36237\n",
      "Gallery Size : 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▍     | 4/9 [1:06:52<1:33:45, 1125.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identification Accuracy : 0.4857\n",
      "(36237,)\n",
      "Num queries : 36237\n",
      "Gallery Size : 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4/9 [1:22:02<1:42:32, 1230.51s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-aa64b9ff6197>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mtest_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/ssd_scratch/cvit/starc52/VoxCeleb2/test/mp4\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.55\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/ssd_scratch/cvit/starc52/LPscheckpoints/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_e49.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m#     dev_acc=run_evaluate(root=\"/ssd_scratch/cvit/starc52/VoxCeleb2/dev/mp4\", loss_factor=0.25, model_path=join('/ssd_scratch/cvit/starc52/LPscheckpoints/', 'model_e47.pth'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-aa64b9ff6197>\u001b[0m in \u001b[0;36mrun_evaluate\u001b[0;34m(root, loss_factor, model_path)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mevaluation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgallery_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_queries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m36237\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0macc_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-5daf9b00386f>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mdistances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mgal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoMatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgalleries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mface_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_emb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoMatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0mdistances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-44a2aec62c5f>\u001b[0m in \u001b[0;36mget_embedding\u001b[0;34m(self, input_path_pair, emb)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0maudio_fft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_fft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mres_face_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_audio_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearnable_pins_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_fft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mres_audio_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres_audio_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mres_face_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres_face_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p3ptch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/audret/imageDistillation/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, face, audio, tau)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p3ptch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/audret/imageDistillation/vggm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0minp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0minp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# inp=self.classifier(inp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p3ptch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p3ptch/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p3ptch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p3ptch/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p3ptch/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_evaluate(root=\"/scratch/starc52/VoxCeleb2/test/mp4/\", loss_factor=0.25, model_path=join('/ssd_scratch/cvit/starc52/LPscheckpoints','model_e49.pth')):\n",
    "    test_root = root\n",
    "    \n",
    "    comb_model=combine_models()\n",
    "\n",
    "    embedder = GetEmbeddings(learnable_pins_model=comb_model, loss_factor=loss_factor)\n",
    "\n",
    "\n",
    "    acc_arr = []\n",
    "    for i in tqdm(range(2, 11)):\n",
    "        evaluation = Evaluation(root=test_root, embedder=embedder,gallery_size=i,num_queries=36237)\n",
    "\n",
    "        acc = evaluation.evaluate()\n",
    "        acc_arr.append(acc)\n",
    "\n",
    "    return acc_arr\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_acc=run_evaluate(root=\"/ssd_scratch/cvit/starc52/VoxCeleb2/test/mp4\", loss_factor=0.55, model_path=join('/ssd_scratch/cvit/starc52/LPscheckpoints/', 'model_e49.pth'))\n",
    "#     dev_acc=run_evaluate(root=\"/ssd_scratch/cvit/starc52/VoxCeleb2/dev/mp4\", loss_factor=0.25, model_path=join('/ssd_scratch/cvit/starc52/LPscheckpoints/', 'model_e47.pth'))\n",
    "    plt.plot(np.arange(0, len(test_acc))+2, test_acc, label=\"test\")\n",
    "    plt.plot(np.arange(0, len(dev_acc))+2, dev_acc, label=\"dev\")\n",
    "    plt.xlabel('Gallery Size')\n",
    "    plt.ylabel('Identification Accuracy')\n",
    "    plt.title('1:N F-V matching')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.savefig('/home/starc52/audret/graphs/epoch_43.png')\n",
    "    plt.clf()\n",
    "    # run_multiple_epochs(root=args.root)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
