{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f657afe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os,sys\n",
    "sys.path.append('../')\n",
    "from os.path import join\n",
    "import glob\n",
    "import  random\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "# from face2speaker_main import Face2Speaker\n",
    "# from Recode.model import DoubleLayerModel, SingleLayerModel\n",
    "from senet import *\n",
    "from model import *\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62172d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_models():    \n",
    "    # getting the VGGFace_VGGM Learnable pins network\n",
    "    model_path=join('/ssd_scratch/cvit/starc52/LPscheckpoints','model_e47.pth')\n",
    "    model = LearnablePINSenetVggVox256()\n",
    "    model.test()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    model.load_state_dict(torch.load(model_path)['model_state_dict'])\n",
    "    \n",
    "    # getting the distilled model for replacing VGG face image branch\n",
    "    distill_model_path=join('/ssd_scratch/cvit/starc52/distill_checkpoints','epoch_9.pth')\n",
    "    distill_model = CompleteDistillationModel(model)\n",
    "    distill_model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    distill_model.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        distill_model = nn.DataParallel(distill_model)\n",
    "    distill_model.load_state_dict(torch.load(distill_model_path)['model_state_dict'])\n",
    "    \n",
    "    # initiating the learnable pins network for distilled image branch weights\n",
    "    comb_model = LearnablePINSdistill()\n",
    "    pre_face_fc = model.module.face_fc.state_dict()\n",
    "    pre_audio_model = model.module.audio_model.state_dict()\n",
    "    pre_audio_fc = model.module.audio_fc.state_dict()\n",
    "    pre_distill_model = distill_model.module.student_model.state_dict()\n",
    "\n",
    "    comb_model_face_fc = comb_model.face_fc.state_dict()\n",
    "    comb_model_audio_model = comb_model.audio_model.state_dict()\n",
    "    comb_model_audio_fc = comb_model.audio_fc.state_dict()\n",
    "    comb_model_distill_model = comb_model.face_model.state_dict()\n",
    "\n",
    "\n",
    "    pre_face_fc = {k: v for k, v in pre_face_fc.items() if k in comb_model_face_fc}\n",
    "    comb_model_face_fc.update(pre_face_fc) \n",
    "    comb_model.face_fc.load_state_dict(comb_model_face_fc)\n",
    "    \n",
    "    pre_audio_model = {k: v for k, v in pre_audio_model.items() if k in comb_model_audio_model}\n",
    "    comb_model_audio_model.update(pre_audio_model) \n",
    "    comb_model.audio_model.load_state_dict(comb_model_audio_model)\n",
    "\n",
    "    pre_audio_fc = {k: v for k, v in pre_audio_fc.items() if k in comb_model_audio_fc}\n",
    "    comb_model_audio_fc.update(pre_audio_fc) \n",
    "    comb_model.audio_fc.load_state_dict(comb_model_audio_fc)\n",
    "\n",
    "    pre_distill_model = {k: v for k, v in pre_distill_model.items() if k in comb_model_distill_model}\n",
    "    comb_model_distill_model.update(pre_distill_model) \n",
    "    comb_model.face_model.load_state_dict(comb_model_distill_model)\n",
    "    # returning the new learnable pins model\n",
    "    return comb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2069f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "class Evaluation():\n",
    "    def __init__(self, \n",
    "                root, \n",
    "                embedder,\n",
    "                num_queries=100, \n",
    "                gallery_size=5, \n",
    "                distance_metric='euclidean'):\n",
    "\n",
    "        self.embedder = embedder\n",
    "        self.root = root\n",
    "        self.gallery_size = gallery_size\n",
    "        self.num_queries = num_queries\n",
    "        self.distance_metric= distance_metric\n",
    "        self.generate_eval_data()\n",
    "        pass\n",
    "\n",
    "    def distance(self,a,b):\n",
    "        if self.distance_metric=='euclidean':\n",
    "            return np.linalg.norm(a-b)\n",
    "        elif self.distance_metric=='cosine':\n",
    "            return spatial.distance.cosine(a,b)\n",
    "\n",
    "    def generate_eval_data(self):\n",
    "        _id_list = sorted(os.listdir(self.root))\n",
    "\n",
    "        queries = []\n",
    "        final_gallery = []\n",
    "\n",
    "        for _id in sorted(os.listdir(self.root)):\n",
    "            _id_path = join(self.root, _id)\n",
    "            for _url in sorted(os.listdir(_id_path)):\n",
    "                _url_path = join(_id_path,_url)\n",
    "                listOfDirs=[f for f in os.listdir(_url_path) if os.path.isdir(join(_url_path, f))]\n",
    "                for dir in sorted(listOfDirs):\n",
    "                    emb_dir = join(_url_path,dir)\n",
    "                    if os.listdir(emb_dir):\n",
    "                        queries.append(join(emb_dir, random.choice(sorted(os.listdir(emb_dir)))))\n",
    "        random.shuffle(queries)\n",
    "        for _idx, query in enumerate(queries[0:self.num_queries]):\n",
    "            _id = query.split(os.sep)[-4]\n",
    "            same_flag = 1\n",
    "\n",
    "            while same_flag:\n",
    "                answer_set = glob.glob(join(self.root, _id, join('*','*.jpg')))\n",
    "\n",
    "                answer = random.choice(answer_set)\n",
    "                if(not answer.split(os.sep)[-2]==query.split(os.sep)[-3]):\n",
    "                    same_flag = 0\n",
    "                \n",
    "            diff_speakers = [i for i in _id_list if i!=_id]\n",
    "            random.shuffle(diff_speakers)\n",
    "\n",
    "            assert not _id in diff_speakers\n",
    "\n",
    "            impostor_gallery = [] \n",
    "            \n",
    "            for imp in diff_speakers[0:self.gallery_size-1]:\n",
    "                imp_embeddings = glob.glob(join(self.root,imp,join('*','*.jpg')))\n",
    "                impostor_gallery.append(random.choice(imp_embeddings))\n",
    "                \n",
    "            impostor_gallery.append(answer)\n",
    "            final_gallery.append(impostor_gallery)\n",
    "            \n",
    "        self.queries = np.array(queries[0:self.num_queries])\n",
    "        self.galleries = np.array(final_gallery[0:self.num_queries])\n",
    "        self.answer = np.array([self.gallery_size-1]*self.num_queries)\n",
    "        # print(\"self.queries\", self.queries)\n",
    "        # print(\"self.galleries\", self.galleries)\n",
    "        print(\"Num queries : %d\"%(len(self.queries)))\n",
    "        print(\"Gallery Size : %d\"%(self.galleries.shape[1]))\n",
    "        pass\n",
    "\n",
    "    def evaluate(self):\n",
    "\n",
    "        test_samples = self.num_queries\n",
    "\n",
    "        result = []\n",
    "        \n",
    "        for _idx, query in enumerate(self.queries[0:test_samples]):\n",
    "            distances=[]\n",
    "            for gal, toMatch in enumerate(self.galleries[_idx]):\n",
    "#                 print(str(_idx)+\" : \"+str(gal), end=\", \")\n",
    "#                 ax1=plt.subplot(1, len(self.galleries[_idx]), gal+1)\n",
    "#                 ax1.imshow(cv2.cvtColor(cv2.imread(toMatch, cv2.IMREAD_UNCHANGED), cv2.COLOR_BGR2RGB))\n",
    "#                 ax1.set_title(\"Pic_\"+str(gal))\n",
    "                face_emb, audio_emb=self.embedder.get_embedding(input_path_pair=(toMatch, query))\n",
    "                distances.append(self.distance(face_emb, audio_emb))\n",
    "#             plt.show()\n",
    "            result.append(np.argmin(distances))\n",
    "#             print(\"Predicted Index\", np.argmin(distances))\n",
    "#             print()\n",
    "            \n",
    "        result = np.array(result)\n",
    "        r = len(np.where(result==self.answer[0:test_samples])[0])\n",
    "        accuracy = r/test_samples\n",
    "        print(\"Identification Accuracy : %.4f\"%(accuracy))\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ada25723",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetEmbeddings():\n",
    "    def __init__(self, \n",
    "                 learnable_pins_model, loss_factor):\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.loss_factor = loss_factor\n",
    "        self.learnable_pins_model = learnable_pins_model \n",
    "        self.learnable_pins_model.to(self.device)\n",
    "#         if torch.cuda.device_count() > 1:\n",
    "#             self.learnable_pins_model = nn.DataParallel(self.learnable_pins_model) \n",
    "# #         self.learnable_pins_model.test()\n",
    "        self.learnable_pins_model.eval()\n",
    "        \n",
    "    def get_embedding(self, input_path_pair=None, emb=None):\n",
    "        if input_path_pair[0] is not None and input_path_pair[1] is not None:\n",
    "            transformToTensor = transforms.ToTensor()         \n",
    "            face_frame = Image.open(input_path_pair[0]).convert('RGB')\n",
    "            audio_fft = np.load(input_path_pair[1])\n",
    "        img_transform = transforms.Compose([\n",
    "            transforms.Resize((int(224 * self.loss_factor), int(224 * self.loss_factor)), interpolation=2),\n",
    "            transforms.Resize((224, 224), interpolation=2),\n",
    "            # transforms.RandomHorizontalFlip(),\n",
    "            # transforms.ColorJitter(brightness=0.5, hue=0.3),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "#         display(face_frame)\n",
    "        face_frame = img_transform(face_frame).unsqueeze(0)\n",
    "        face_frame = face_frame.to(self.device)\n",
    "\n",
    "        audio_fft = transformToTensor(audio_fft).unsqueeze(0)\n",
    "        audio_fft = audio_fft.to(self.device)\n",
    "        \n",
    "        # face_emb /= np.linalg.norm(face_emb)\n",
    "        # face_emb = torch.from_numpy(face_emb).unsqueeze(0)\n",
    "        # face_emb = face_emb.to(self.device)\n",
    "\n",
    "\n",
    "        # audio_emb /= np.linalg.norm(audio_emb)\n",
    "        # audio_emb = torch.from_numpy(audio_emb).unsqueeze(0)\n",
    "        # audio_emb = audio_emb.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            res_face_emb, res_audio_emb = self.learnable_pins_model(face_frame, audio_fft)\n",
    "        res_audio_emb = res_audio_emb.cpu().numpy().reshape(-1)\n",
    "        res_face_emb = res_face_emb.cpu().numpy().reshape(-1)\n",
    "\n",
    "        return res_face_emb, res_audio_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df891518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "senet\n",
      "Weights Loaded!\n",
      "2048\n",
      "Weights Loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num queries : 1000\n",
      "Gallery Size : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 1/9 [00:19<02:38, 19.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identification Accuracy : 0.7260\n",
      "Num queries : 1000\n",
      "Gallery Size : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 2/9 [00:49<02:58, 25.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identification Accuracy : 0.5900\n",
      "Num queries : 1000\n",
      "Gallery Size : 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 3/9 [01:27<03:07, 31.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identification Accuracy : 0.4530\n",
      "Num queries : 1000\n",
      "Gallery Size : 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▍     | 4/9 [02:16<03:11, 38.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identification Accuracy : 0.3820\n",
      "Num queries : 1000\n",
      "Gallery Size : 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▌    | 5/9 [03:14<03:01, 45.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identification Accuracy : 0.3050\n",
      "Num queries : 1000\n",
      "Gallery Size : 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 6/9 [04:22<02:38, 52.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identification Accuracy : 0.2860\n",
      "Num queries : 1000\n",
      "Gallery Size : 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████▊  | 7/9 [05:38<02:01, 60.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identification Accuracy : 0.2690\n",
      "Num queries : 1000\n",
      "Gallery Size : 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|████████▉ | 8/9 [07:05<01:08, 68.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identification Accuracy : 0.2220\n",
      "Num queries : 1000\n",
      "Gallery Size : 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [08:47<00:00, 58.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identification Accuracy : 0.2040\n",
      "senet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights Loaded!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/ssd_scratch/cvit/starc52/LPscheckpoints/model_e47.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3f7d618b0063>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mtest_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/ssd_scratch/cvit/starc52/VoxCeleb2/test/mp4\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/ssd_scratch/cvit/starc52/LPscheckpoints/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_e47.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mdev_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/ssd_scratch/cvit/starc52/VoxCeleb2/dev/mp4\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/ssd_scratch/cvit/starc52/LPscheckpoints/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_e47.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dev\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-3f7d618b0063>\u001b[0m in \u001b[0;36mrun_evaluate\u001b[0;34m(root, loss_factor, model_path)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#         model = nn.DataParallel(model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#     model.load_state_dict(torch.load(model_path)['model_state_dict'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcomb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcombine_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0membedder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGetEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearnable_pins_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-2a1568a880d6>\u001b[0m in \u001b[0;36mcombine_models\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# getting the distilled model for replacing VGG face image branch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p3ptch/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p3ptch/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/p3ptch/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/ssd_scratch/cvit/starc52/LPscheckpoints/model_e47.pth'"
     ]
    }
   ],
   "source": [
    "def run_evaluate(root=\"/scratch/starc52/VoxCeleb2/test/mp4/\", loss_factor=0.25, model_path=join('/ssd_scratch/cvit/starc52/LPscheckpoints','model_e49.pth')):\n",
    "    test_root = root\n",
    "\n",
    "#     model = LearnablePINSenetVggVox256()\n",
    "#     model.test()\n",
    "#     if torch.cuda.device_count() > 1:\n",
    "#         model = nn.DataParallel(model)\n",
    "#     model.load_state_dict(torch.load(model_path)['model_state_dict'])\n",
    "    comb_model=combine_models()\n",
    "\n",
    "    embedder = GetEmbeddings(learnable_pins_model=comb_model, loss_factor=loss_factor)\n",
    "\n",
    "\n",
    "    acc_arr = []\n",
    "    for i in tqdm(range(2, 11)):\n",
    "        evaluation = Evaluation(root=test_root, embedder=embedder,gallery_size=i,num_queries=1000)\n",
    "\n",
    "        acc = evaluation.evaluate()\n",
    "        acc_arr.append(acc)\n",
    "\n",
    "    return acc_arr\n",
    "\n",
    "\n",
    "def run_multiple_epochs(root=\"/scratch/starc52/VoxCeleb2/\", model_path = \"/ssd_scratch/cvit/starc52/LPscheckpoints/\"):\n",
    "    listOfPaths = ['test/mp4', 'dev/mp4']\n",
    "    listOfModels= [os.path.join(model_path, \"model_e\"+str(num)+\".pth\") for num in range(30, 50)]\n",
    "    listOfAccuracies={}\n",
    "    for idx, path in enumerate(tqdm(listOfModels)):\n",
    "        acc_arr={}\n",
    "        for pathAdd in listOfPaths:\n",
    "            print(\"set, model:\", os.path.join(root, pathAdd), path)\n",
    "            acc_arr[pathAdd.split('/')[0]] = run_evaluate(root=os.path.join(root, pathAdd), model_path=path)\n",
    "            listOfAccuracies[path.replace(\"/\", \"-\")] = acc_arr\n",
    "            print(acc_arr)\n",
    "    os.makedirs(\"/home/starc52/audret/graphs\", exist_ok=True)\n",
    "    for model in listOfAccuracies:\n",
    "        for dataset in model:\n",
    "            plt.plot(np.arange(0, len(listOfAccuracies[model[dataset]]))+2, listOfAccuracies[model[dataset]], label=dataset)\n",
    "        plt.xlabel('Gallery Size')\n",
    "        plt.ylabel('Identification Accuracy')\n",
    "        plt.title('1:N F-V matching')\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.savefig('/home/starc52/audret/graphs/'+str(model)+'_nor.png')\n",
    "        plt.clf()\n",
    "    # maxList = max(listOfAccuracies, key=lambda x: x[0])\n",
    "    # maxPath=[listOfModels[listOfAccuracies.index(maxList)]]\n",
    "    # print(list(zip(listOfModels, listOfAccuracies)))\n",
    "    # print(list(zip(maxPath, [maxList])))\n",
    "    # for idx, path in enumerate(maxPath):\n",
    "    #     plt.plot(np.arange(0,len(maxList))+2,maxList, label=path[-7:-4])\n",
    "    # plt.xlabel('Gallery Size')\n",
    "    # plt.ylabel('Identification Accuracy')\n",
    "    # plt.title('1:N F-V matching')\n",
    "    # plt.grid()\n",
    "    # plt.legend()\n",
    "    # plt.savefig('/home/starc52/audret/retrieve_'+root.split(\"/\")[-2]+'_graph.png')\n",
    "    # plt.clf()\n",
    "    # plt.show()\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--root',default='/scratch/starc52/VoxCeleb2/', type=str)\n",
    "# args = parser.parse_args()\n",
    "if __name__ == '__main__':\n",
    "    test_acc=run_evaluate(root=\"/ssd_scratch/cvit/starc52/VoxCeleb2/test/mp4\", loss_factor=0.25, model_path=join('/ssd_scratch/cvit/starc52/LPscheckpoints/', 'model_e47.pth'))\n",
    "    dev_acc=run_evaluate(root=\"/ssd_scratch/cvit/starc52/VoxCeleb2/dev/mp4\", loss_factor=0.25, model_path=join('/ssd_scratch/cvit/starc52/LPscheckpoints/', 'model_e47.pth'))\n",
    "    plt.plot(np.arange(0, len(test_acc))+2, test_acc, label=\"test\")\n",
    "    plt.plot(np.arange(0, len(dev_acc))+2, dev_acc, label=\"dev\")\n",
    "    plt.xlabel('Gallery Size')\n",
    "    plt.ylabel('Identification Accuracy')\n",
    "    plt.title('1:N F-V matching')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.savefig('/home/starc52/audret/graphs/epoch_43.png')\n",
    "    plt.clf()\n",
    "    # run_multiple_epochs(root=args.root)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
